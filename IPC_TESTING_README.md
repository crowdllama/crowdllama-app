# CrowdLLaMA IPC Testing Guide

This guide provides comprehensive testing for the IPC (Inter-Process Communication) functionality between React frontend, Rust backend, and Unix socket communication.

## ğŸ—ï¸ Architecture Overview

```
React Frontend â†” Tauri Commands â†” Rust Backend â†” Unix Socket â†” External Processes
```

- **React â†’ Rust**: JSON messages via Tauri commands
- **Rust â†’ IPC**: Protobuf messages via Unix domain socket
- **IPC â†’ Rust**: Protobuf messages received and parsed

## ğŸ§ª Testing Components

### 1. Rust Integration Tests (`src-tauri/tests/ipc_integration_test.rs`)
- **Purpose**: Test Unix socket communication with protobuf messages
- **Tests**:
  - `test_ipc_generate_request_response_flow()`: Full GenerateRequest/Response flow
  - `test_ipc_invalid_message_handling()`: Error handling for invalid messages
  - `test_ipc_windows_stub()`: Windows compatibility test
  - `test_socket_listener_creation()`: Basic SocketListener creation

### 2. React Frontend Interface (`src/App.tsx`)
- **Purpose**: User interface for testing React â†” Rust communication
- **Features**:
  - Send GenerateRequest to IPC socket
  - Send GenerateResponse to IPC socket
  - Simulate receiving messages from IPC
  - Real-time message logging
  - Console logging for debugging

### 3. Python Test Script (`test_ipc.py`)
- **Purpose**: External testing of Unix socket communication
- **Features**:
  - Direct socket connection testing
  - Message length prefix handling
  - Batch message testing
  - Error handling and diagnostics

## ğŸš€ How to Run Tests

### Prerequisites
```bash
# Ensure system dependencies are installed (Linux only)
sudo apt-get install -y libwebkit2gtk-4.1-dev build-essential curl wget libssl-dev libgtk-3-dev

# Install Rust and Node.js dependencies
cd src-tauri
cargo build
cd ..
npm install  # or bun install
```

### 1. Run Rust Integration Tests
```bash
cd src-tauri
export PATH="/usr/local/cargo/bin:$PATH"
cargo test ipc_integration_test --lib -- --nocapture
```

**Expected Output:**
- âœ… Socket creation and connection
- âœ… Message sending with length prefixes
- âœ… Protobuf encoding/decoding
- âœ… Multiple message handling

### 2. Start the Tauri Application
```bash
# Terminal 1: Start the development server
npm run tauri dev
# or
bun run tauri dev
```

**Look for these logs:**
```
ğŸš€ CrowdLLaMA IPC Testing Interface Ready
Socket listener started on /tmp/crowdllama.sock
```

### 3. Test React Frontend Interface
1. **Open the Tauri application** (should open automatically)
2. **Navigate to the IPC Testing section**
3. **Test the buttons**:
   - `ğŸ“¤ Send GenerateRequest`: Tests React â†’ Rust â†’ IPC flow
   - `ğŸ“¤ Send GenerateResponse`: Tests response message flow
   - `ğŸ“¨ Simulate Receive`: Tests IPC â†’ Rust â†’ React flow
   - `ğŸ—‘ï¸ Clear Messages`: Clears the log

**Expected Behavior:**
- Messages appear in the UI log
- Console logs show detailed message flow
- Rust terminal shows received protobuf messages

### 4. Test with Python Script
```bash
# Terminal 2: Run the external test script
python3 test_ipc.py
```

**Expected Output:**
```
ğŸ§ª CrowdLLaMA IPC Socket Test Script
ğŸ”Œ Attempting to connect to IPC socket...
âœ… Connected to /tmp/crowdllama.sock
ğŸ“¤ Test 1: Sending GenerateRequest
ğŸ“¤ Sent message: 123 bytes
ğŸ“¤ Test 2: Sending GenerateResponse
ğŸ“¤ Sent message: 145 bytes
ğŸ“¤ Test 3: Sending batch messages
âœ… All test messages sent successfully!
```

### 5. Test with cURL (Advanced)
```bash
# Test socket connectivity (this will fail but shows socket exists)
echo "test" | nc -U /tmp/crowdllama.sock

# Check socket file exists
ls -la /tmp/crowdllama.sock
```

## ğŸ“Š Expected Log Outputs

### Rust Backend Logs
```
Socket listener started on /tmp/crowdllama.sock
New connection accepted
ğŸ“ Reading message of length: 45 bytes
âœ… Received protobuf message: ğŸ”„ GenerateRequest: model=test-model, prompt=Hello from React!, stream=false
ğŸ”„ Received message from React: GenerateRequest { data: JsonGenerateRequest { model: "test-model", prompt: "Hello from React!", stream: false } }
ğŸ“¦ Encoded protobuf message: 45 bytes
ğŸ“¤ Sent 45 bytes to IPC socket
âœ… Successfully sent message to IPC socket
```

### React Frontend Logs (Browser Console)
```
ğŸš€ CrowdLLaMA IPC Testing Interface Ready
ğŸ“¤ Sending GenerateRequest to Rust: {type: "GenerateRequest", data: {model: "test-model", prompt: "Hello from React!", stream: false}}
âœ… Result from Rust: Message sent successfully
ğŸ“¨ Simulating receive message from IPC
ğŸ“¨ Received message from Rust: {type: "GenerateResponse", data: {model: "simulated-model", response: "This is a simulated response from the IPC system", done: true, worker_id: "worker-sim-123"}}
```

### Python Script Logs
```
ğŸ”Œ Attempting to connect to IPC socket...
âœ… Connected to /tmp/crowdllama.sock
ğŸ“¤ Test 1: Sending GenerateRequest
ğŸ“¤ Sent message: 123 bytes
ğŸ“¤ Test 2: Sending GenerateResponse  
ğŸ“¤ Sent message: 145 bytes
ğŸ‰ Test completed successfully!
```

## ğŸ› Troubleshooting

### Common Issues

#### 1. Socket Not Found
```
âŒ Socket file not found: /tmp/crowdllama.sock
```
**Solution**: Ensure the Tauri application is running and IPC listener started.

#### 2. Connection Refused
```
âŒ Connection refused to: /tmp/crowdllama.sock
```
**Solution**: Check that the socket listener is running and not crashed.

#### 3. Permission Denied
```
âŒ Permission denied: /tmp/crowdllama.sock
```
**Solution**: Check file permissions or run with appropriate user.

#### 4. Windows Compatibility
```
âš ï¸ Unix domain sockets are not supported on Windows
```
**Expected**: Windows uses stub implementation for testing.

### Debugging Steps

1. **Check socket file exists**:
   ```bash
   ls -la /tmp/crowdllama.sock
   ```

2. **Monitor socket connections**:
   ```bash
   sudo netstat -ax | grep crowdllama
   ```

3. **Check Tauri logs**:
   - Look for "Socket listener started" message
   - Check for any error messages in the terminal

4. **Verify protobuf encoding**:
   - Check byte lengths in logs
   - Verify message structure in Rust output

## ğŸ” Message Flow Verification

### Complete Flow Test
1. **Start Tauri app** â†’ Socket created
2. **Click "Send GenerateRequest"** â†’ React â†’ Rust â†’ Socket
3. **Run Python script** â†’ External â†’ Socket â†’ Rust logs
4. **Click "Simulate Receive"** â†’ Rust â†’ React â†’ UI update

### Success Indicators
- âœ… Socket file created at `/tmp/crowdllama.sock`
- âœ… Messages appear in all three logs (React, Rust, Python)
- âœ… Protobuf encoding/decoding works correctly
- âœ… Length prefixes handled properly
- âœ… No connection errors or timeouts

## ğŸ“ Test Results Template

```
## Test Run Results

**Date**: [DATE]
**Platform**: [Linux/macOS/Windows]
**Rust Version**: [VERSION]
**Node Version**: [VERSION]

### Test Results
- [ ] Rust integration tests pass
- [ ] Tauri app starts successfully
- [ ] Socket file created
- [ ] React buttons work
- [ ] Python script connects
- [ ] Messages logged correctly
- [ ] No errors in console

### Issues Found
[List any issues or errors encountered]

### Notes
[Additional observations or comments]
```

## ğŸ¯ Next Steps

After successful testing:
1. **Implement real protobuf communication** (replace JSON test messages)
2. **Add bidirectional message handling** (IPC â†’ React notifications)
3. **Implement message queuing** for high-volume scenarios
4. **Add authentication/security** for production use
5. **Create performance benchmarks** for message throughput